{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LDA_post.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"6IasXaTg84x_","colab_type":"code","colab":{}},"source":["import string\n","import re, nltk, spacy, gensim\n","import pandas as pd\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","\n","import matplotlib.pyplot as plt\n","\n","pd.options.mode.chained_assignment = None\n","tr = str.maketrans(\"\", \"\", string.punctuation)\n","stemmer = SnowballStemmer(\"english\")\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4q_wwOz084yD","colab_type":"code","colab":{}},"source":["#load customized stopwords \n","stopword_c = pd.read_csv('../Dataset/results/stopword.csv', encoding = \"ISO-8859-1\")\n","stopword_c = stopword_c[stopword_c['stopword'] == 1]\n","stopword_c = stopword_c['term'].tolist()\n","\n","# load nltk stopwords\n","stoplist = stopwords.words('english')\n","stoplist.extend(stopword_c)\n","stoplist.extend(['be', 'have', 'not', 'do', 'so', 'when', 'would', 'that', 'can', 'more'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTatAvwW84yF","colab_type":"code","colab":{}},"source":["# load classfied dataset\n","df = pd.read_csv('../Dataset/results/esre_5000_sgd_clf_result.csv')\n","# dataframe for emotional and non-emotional eating\n","df_post_esre = df[df['esre'] == 1].reset_index(drop = True, inplace = False)\n","df_post_nesre = df[df['esre'] == 0].reset_index(drop = True, inplace = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPlNrhoQ84yI","colab_type":"code","colab":{}},"source":["# preprocess sentences in posts(remove puctuation marks, stemming)\n","def preprocess_posts(df):\n","    nltk.download('punkt')\n","    df['unpunct_body'] = df.selftext.apply(lambda x: x.translate(tr))\n","    df['tokenized_body'] = df.unpunct_body.apply(lambda x: nltk.word_tokenize(x))\n","    df['lower_and_tokenized_body'] = df.tokenized_body.apply(lambda x: [y.lower() for y in x])\n","    df['stemmed_text'] = df.lower_and_tokenized_body.apply(lambda x: [stemmer.stem(y) for y in x])\n","    df['document'] = df.stemmed_text.map(lambda x: ' '.join([y for y in x]))\n","    return df\n","\n","# stopword removal \n","def remove_stopwords(wordlist, stopwords):\n","    return [w for w in wordlist if w not in stopwords]\n","\n","# tokenize preprocessed sentences(by preprocess_posts() function) \n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n","\n","# lemmatizing tokenzed words(noun, adjective, verb and adverb only)\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n","    return texts_out\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","def preprocess_document(df):\n","    data_words = list(sent_to_words(df['document'].tolist()))\n","    data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","    print(data_lemmatized[:2])\n","    return data_lemmatized"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlxtfB1284yO","colab_type":"code","colab":{}},"source":["# preprocess posts (emotional/non-emotional)\n","topic_post_esre = preprocess_posts(df_post_esre)\n","post_esre = preprocess_document(topic_post_esre)\n","del topic_post_esre\n","# topic_post_nesre = preprocess_posts(df_post_nesre)\n","# post_nesre = preprocess_document(topic_post_nesre)\n","# del topic_post_nesre"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUETvzti84yT","colab_type":"code","colab":{}},"source":["# count-vectorize posts based on term frequency\n","def vectorize_document(data_lemmatized):\n","    vectorizer = CountVectorizer(stop_words=stoplist, min_df = 10, ngram_range =(1, 3))\n","    data_vectorized = vectorizer.fit_transform(data_lemmatized)\n","    return vectorizer, data_vectorized"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckLMrqjg84yV","colab_type":"code","colab":{}},"source":["vectorizer_post_esre,data_vectorized_post_esre = vectorize_document(post_esre)\n","# vectorizer_post_nesre,data_vectorized_post_nesre = vectorize_document(post_nesre)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzncvlfz84yX","colab_type":"code","colab":{}},"source":["# build LDA model\n","def buildLDA(data_vectorized, i):\n","    # Build LDA Model\n","    lda_model = LatentDirichletAllocation(n_components=i,               # Number of topics\n","                                          max_iter=10,               # Max learning iterations\n","                                          learning_method='online',   \n","                                          random_state=100,          # Random state\n","                                          batch_size=128,            # n docs in each learning iter\n","                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n","                                          n_jobs = 1               # Use all available CPUs\n","                                         )\n","    lda_output = lda_model.fit_transform(data_vectorized)\n","\n","    print(lda_model)  # Model attributes\n","    print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n","    print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n","    print(lda_model.get_params())\n","    return lda_model, lda_output\n","\n","# label documents(posts) with their topic\n","def get_document_topic(best_lda_model, data_vectorized, data):\n","    # Create Document - Topic Matrix\n","    lda_output = best_lda_model.transform(data_vectorized)\n","\n","    # column names\n","    topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n","\n","    # index names\n","    docnames = [\"Doc\" + str(i) for i in range(len(data))]\n","\n","    # Make the pandas dataframe\n","    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n","\n","    # Get dominant topic for each document\n","    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n","    df_document_topic['dominant_topic'] = dominant_topic\n","    df_document_topic['id'] = df_post_esre.id.tolist()\n","    df_document_topic['title'] = df_post_esre.title.tolist()\n","    df_document_topic['selftext'] = df_post_esre.selftext.tolist()\n","    \n","\n","    # Styling\n","    def color_green(val):\n","        color = 'green' if val > .1 else 'black'\n","        return 'color: {col}'.format(col=color)\n","\n","    def make_bold(val):\n","        weight = 700 if val > .1 else 400\n","        return 'font-weight: {weight}'.format(weight=weight)\n","\n","    # Apply Style\n","    df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n","    print(df_document_topics)\n","    return df_document_topic\n","\n","# label documents(posts) with their topic(for non-emotional eating)\n","def get_document_topic_nesre(best_lda_model, data_vectorized, data):\n","    # Create Document - Topic Matrix\n","    lda_output = best_lda_model.transform(data_vectorized)\n","\n","    # column names\n","    topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n","\n","    # index names\n","    docnames = [\"Doc\" + str(i) for i in range(len(data))]\n","\n","    # Make the pandas dataframe\n","    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n","\n","    # Get dominant topic for each document\n","    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n","    df_document_topic['dominant_topic'] = dominant_topic\n","    df_document_topic['id'] = df_post_nesre.id.tolist()\n","    df_document_topic['title'] = df_post_nesre.title.tolist()\n","    df_document_topic['selftext'] = df_post_nesre.selftext.tolist()\n","    \n","\n","    # Styling\n","    def color_green(val):\n","        color = 'green' if val > .1 else 'black'\n","        return 'color: {col}'.format(col=color)\n","\n","    def make_bold(val):\n","        weight = 700 if val > .1 else 400\n","        return 'font-weight: {weight}'.format(weight=weight)\n","\n","    # Apply Style\n","    df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n","    print(df_document_topics)\n","    return df_document_topic\n","\n","# get distribution data of topics \n","def get_df_topic_distribution(df_document_topic):\n","    df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n","    df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n","    print(df_topic_distribution)\n","    return df_topic_distribution\n","\n","# visualize \n","def get_visualization(best_lda_model, data_vectorized, vectorizer):\n","    pyLDAvis.enable_notebook()\n","    panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne', R=50)\n","    pyLDAvis.save_html(panel, '../Dataset/results/lda/lda_' + str(number_of_topics) + '.html')\n","        \n","    #     print(panel)\n","    return panel\n","\n","# visualize (for non-emotional eating)\n","def get_visualization_nesre(best_lda_model, data_vectorized, vectorizer):\n","    pyLDAvis.enable_notebook()\n","    panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne', R=50)\n","    pyLDAvis.save_html(panel, '../Dataset/results/lda/lda_' + str(number_of_topics) + '.html')\n","        \n","    #     print(panel)\n","    return panel\n","\n","\n","# Show top n keywords for each topic\n","def show_topics(vectorizer, lda_model, n_words=20):\n","    keywords = np.array(vectorizer.get_feature_names())\n","    topic_keywords = []\n","    for topic_weights in lda_model.components_:\n","        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n","        topic_keywords.append(keywords.take(top_keyword_locs))\n","    return topic_keywords\n","\n","def get_topic_keywords_matrix(vectorizer, best_lda_model):\n","    topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=100)        \n","\n","    # Topic - Keywords Dataframe\n","    df_topic_keywords = pd.DataFrame(topic_keywords)\n","    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n","    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n","    print(df_topic_keywords.head())\n","    return df_topic_keywords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FR_YKsfY84yZ","colab_type":"code","colab":{}},"source":["def lda_model_nesre(data_vectorized, number_of_topics, df):\n","        lda_model_post_nesre, lda_output_post_nesre = buildLDA(data_vectorized, number_of_topics)\n","        df_document_topic_post_nesre = get_document_topic_nesre(lda_model_post_nesre, data_vectorized, df)\n","        df_topic_distribution_post_nesre = get_df_topic_distribution(df_document_topic_post_nesre)\n","        get_visualization_nesre(lda_model_post_nesre, data_vectorized, vectorizer_post_nesre)\n","        topic_keyword_matrix_post_nesre = get_topic_keywords_matrix(vectorizer_post_nesre, lda_model_post_nesre)\n","\n","        print(df_document_topic_post_nesre.head())\n","        df_document_topic_post_nesre.to_csv('../Dataset/results/lda/df_document_topic_' + str(number_of_topics) + '.csv')\n","        df_topic_distribution_post_nesre.to_csv('../Dataset/results/lda/df_topic_distribution' + str(number_of_topics) + '.csv')\n","        topic_keyword_matrix_post_nesre.to_csv('../Dataset/results/lda/topic_keyword_matrix' + str(number_of_topics) + '.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSi_7p-N84yb","colab_type":"code","colab":{}},"source":["# build lda model and get topic modeling results\n","for number_of_topics  in range(4, 6):\n","    lda_model_esre(data_vectorized_post_esre, number_of_topics, post_esre)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtfxeZCd84yg","colab_type":"code","colab":{}},"source":["df = pd.read_csv('../Dataset/results/lda/post_esre/df_document_topic_post_esre_4.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNDtlHRY84yi","colab_type":"code","colab":{}},"source":["df[:200].to_csv('../Dataset/results/lda/post_esre/df_document_topic_post_esre_4_200.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRXOt8UQ84ym","colab_type":"code","colab":{}},"source":["df = pd.read_csv('../Dataset/results/lda/post_esre/df_document_topic_post_esre_4.csv')[0:200]\n","del df['Unnamed: 0']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acm3CTE984yo","colab_type":"code","colab":{}},"source":["df.to_csv('../Dataset/results/lda/post_esre/df_document_topic_post_esre_4_200.csv', index = False)"],"execution_count":0,"outputs":[]}]}